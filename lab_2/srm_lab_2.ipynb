{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aSL1rAMGqpGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a139ba-8675-4a9a-dd8b-69cb7422e6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 10 samples...\n",
            "\n",
            "====================================================================================================\n",
            "Starting Self-Learning Algorithm\n",
            "Data size: 10\n",
            "\n",
            "Initial estimated means: [-0.17220465  0.92623592  2.02467649  3.12311707]\n",
            "Initial estimated probabilities: [0.25 0.25 0.25 0.25]\n",
            "Convergence reached after 78 iterations.\n",
            "Algorithm completed in 78 iterations.\n",
            "True means: [0 1 2 3]\n",
            "True probabilities: [0.1 0.2 0.3 0.4]\n",
            "\n",
            "Estimated parameters:\n",
            "Means: [0.61 2.15 2.15 2.15]\n",
            "Probabilities: [0.13 0.19 0.37 0.31]\n",
            "\n",
            "Differences:\n",
            "Means difference: [0.61174247 1.14556935 0.14556953 0.85443047]\n",
            "Probabilities difference: [0.02888341 0.01192288 0.06948261 0.08644314]\n",
            "\n",
            "\n",
            "Generating 100 samples...\n",
            "\n",
            "====================================================================================================\n",
            "Starting Self-Learning Algorithm\n",
            "Data size: 100\n",
            "\n",
            "Initial estimated means: [-1.2103676   1.04867791  3.30772342  5.56676893]\n",
            "Initial estimated probabilities: [0.25 0.25 0.25 0.25]\n",
            "Convergence reached after 238 iterations.\n",
            "Algorithm completed in 238 iterations.\n",
            "True means: [0 1 2 3]\n",
            "True probabilities: [0.1 0.2 0.3 0.4]\n",
            "\n",
            "Estimated parameters:\n",
            "Means: [0.62 0.62 2.2  4.48]\n",
            "Probabilities: [0.12 0.23 0.57 0.08]\n",
            "\n",
            "Differences:\n",
            "Means difference: [0.6241041  0.3754365  0.19883677 1.4781775 ]\n",
            "Probabilities difference: [0.0163117  0.03116069 0.2721245  0.31959689]\n",
            "\n",
            "\n",
            "Generating 1000 samples...\n",
            "\n",
            "====================================================================================================\n",
            "Starting Self-Learning Algorithm\n",
            "Data size: 1000\n",
            "\n",
            "Initial estimated means: [-3.01674087  0.06312249  3.14298584  6.22284919]\n",
            "Initial estimated probabilities: [0.25 0.25 0.25 0.25]\n",
            "Convergence reached after 1610 iterations.\n",
            "Algorithm completed in 1610 iterations.\n",
            "True means: [0 1 2 3]\n",
            "True probabilities: [0.1 0.2 0.3 0.4]\n",
            "\n",
            "Estimated parameters:\n",
            "Means: [-0.23  0.91  2.56  3.69]\n",
            "Probabilities: [0.08 0.3  0.54 0.08]\n",
            "\n",
            "Differences:\n",
            "Means difference: [0.22974327 0.08711338 0.56433319 0.68813154]\n",
            "Probabilities difference: [0.01619591 0.10279914 0.23782357 0.3244268 ]\n",
            "\n",
            "\n",
            "Generating 5000 samples...\n",
            "\n",
            "====================================================================================================\n",
            "Starting Self-Learning Algorithm\n",
            "Data size: 5000\n",
            "\n",
            "Initial estimated means: [-2.81104566  0.39088459  3.59281483  6.79474508]\n",
            "Initial estimated probabilities: [0.25 0.25 0.25 0.25]\n",
            "Convergence reached after 2005 iterations.\n",
            "Algorithm completed in 2005 iterations.\n",
            "True means: [0 1 2 3]\n",
            "True probabilities: [0.1 0.2 0.3 0.4]\n",
            "\n",
            "Estimated parameters:\n",
            "Means: [0.22 1.51 2.83 4.85]\n",
            "Probabilities: [0.16 0.31 0.53 0.01]\n",
            "\n",
            "Differences:\n",
            "Means difference: [0.21979367 0.51380881 0.82701558 1.85093633]\n",
            "Probabilities difference: [0.05705654 0.10854793 0.22910925 0.39471372]\n",
            "\n",
            "\n",
            "Generating 10000 samples...\n",
            "\n",
            "====================================================================================================\n",
            "Starting Self-Learning Algorithm\n",
            "Data size: 10000\n",
            "\n",
            "Initial estimated means: [-3.37552241 -0.11721528  3.14109185  6.39939898]\n",
            "Initial estimated probabilities: [0.25 0.25 0.25 0.25]\n",
            "Convergence reached after 10044 iterations.\n",
            "Algorithm completed in 10044 iterations.\n",
            "True means: [0 1 2 3]\n",
            "True probabilities: [0.1 0.2 0.3 0.4]\n",
            "\n",
            "Estimated parameters:\n",
            "Means: [-0.07  1.08  2.38  3.19]\n",
            "Probabilities: [0.09 0.27 0.39 0.24]\n",
            "\n",
            "Differences:\n",
            "Means difference: [0.073527   0.08291647 0.37855811 0.19218913]\n",
            "Probabilities difference: [0.0078742  0.07127826 0.09239571 0.15579977]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "def self_learning_algorithm(true_means: np.ndarray, true_probabilities: np.ndarray, data_size: int, data: np.ndarray, tolerance: float = 1e-3, max_iterations: int = 20000) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Self-learning algorithm to estimate mixture model parameters using Expectation-Maximization (EM).\n",
        "\n",
        "    Args:\n",
        "        true_means (np.ndarray): True means of the components.\n",
        "        true_probabilities (np.ndarray): True probabilities of the components.\n",
        "        data_size (int): Number of data points.\n",
        "        data (np.ndarray): Observed data.\n",
        "        tolerance (float, optional): Convergence tolerance for parameter updates. Defaults to 1e-3.\n",
        "        max_iterations (int, optional): Maximum number of iterations. Defaults to 20,000.\n",
        "\n",
        "    Returns:\n",
        "        tuple[np.ndarray, np.ndarray]: Estimated probabilities and means of the components.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"Starting Self-Learning Algorithm\\nData size: {data_size}\\n\")\n",
        "\n",
        "    # Initialize means and probabilities\n",
        "    estimated_means = np.linspace(min(data), max(data), 4)\n",
        "    estimated_probabilities  = np.ones(4) / 4\n",
        "\n",
        "    print(f\"Initial estimated means: {estimated_means}\")\n",
        "    print(f\"Initial estimated probabilities: {estimated_probabilities }\")\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        # E-Step: Calculate responsibilities (gamma)\n",
        "        responsibilities = np.zeros((data_size, 4))\n",
        "        for k in range(4):\n",
        "            responsibilities[:, k] = estimated_probabilities[k] * norm.pdf(data, loc=estimated_means[k], scale=1)\n",
        "        responsibilities /= np.maximum(responsibilities.sum(axis=1, keepdims=True), 1e-10)\n",
        "\n",
        "        # M-Step: Update means and probabilities\n",
        "        new_means = np.sum(responsibilities * data[:, np.newaxis], axis=0) / (responsibilities.sum(axis=0) + 1e-10)\n",
        "        new_probabilities = responsibilities.sum(axis=0) / data_size\n",
        "\n",
        "        # Check convergence\n",
        "        if np.all(np.abs(new_probabilities - estimated_probabilities) < 1e-5) and np.all(np.abs(new_means - estimated_means) < tolerance):\n",
        "            print(f\"Convergence reached after {iteration + 1} iterations.\")\n",
        "            break\n",
        "\n",
        "        estimated_means, estimated_probabilities = new_means, new_probabilities\n",
        "\n",
        "    print(f\"Algorithm completed in {iteration + 1} iterations.\")\n",
        "    print(f\"True means: {true_means}\")\n",
        "    print(f\"True probabilities: {true_probabilities}\")\n",
        "    print(\"\\nEstimated parameters:\")\n",
        "    print(f\"Means: {estimated_means.round(2)}\")\n",
        "    print(f\"Probabilities: {estimated_probabilities.round(2)}\")\n",
        "    print(\"\\nDifferences:\")\n",
        "    print(f\"Means difference: {np.abs(true_means - estimated_means)}\")\n",
        "    print(f\"Probabilities difference: {np.abs(true_probabilities - estimated_probabilities)}\\n\")\n",
        "\n",
        "    return estimated_probabilities , estimated_means\n",
        "\n",
        "\n",
        "sample_sizes = [10, 100, 1000, 5000, 10000]\n",
        "true_means = np.array([0, 1, 2, 3])\n",
        "true_probabilities = np.array([(k + 1) / 10 for k in true_means])\n",
        "true_probabilities /= true_probabilities.sum()\n",
        "\n",
        "# Run the algorithm for different data sizes\n",
        "for size in sample_sizes:\n",
        "    print(f\"\\nGenerating {size} samples...\\n\")\n",
        "\n",
        "    # Generate sample data\n",
        "    sampled_means = np.random.choice(true_means, size=size, p=true_probabilities)\n",
        "    generated_data = np.random.normal(loc=sampled_means, scale=1, size=size)\n",
        "\n",
        "    # Apply the self-learning algorithm\n",
        "    final_probabilities, final_means = self_learning_algorithm(true_means=true_means, true_probabilities=true_probabilities, data_size=size, data=generated_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Алгоритм показує значну залежність від кількості даних n та початкових оцінок параметрів. Зі збільшенням обсягу даних n, його результати стають більш точними та стабільними. Для невеликих значень n (наприклад, n = 10) алгоритм швидко збігається, але результати мають високу варіативність і значні відхилення від істинних значень через обмеженість спостережень. Середні значення та ймовірності для таких обсягів даних оцінюються ненадійно.\n",
        "\n",
        "Для середніх значень n (наприклад, n = 100) точність зростає, але алгоритм все ще демонструє певні похибки, особливо у визначенні ймовірностей. Великі n (наприклад, n = 5000 або n = 10000) дозволяють алгоритму значно зменшити похибки оцінок середніх і ймовірностей. Водночас зростає кількість ітерацій, необхідних для досягнення зближення, оскільки більший обсяг даних потребує більше обчислень для оновлення параметрів.\n",
        "\n",
        "Початкові оцінки параметрів також впливають на швидкість і точність алгоритму. Якщо початкові значення середніх mu_k або ймовірностей p_k значно відрізняються від істинних, алгоритму потрібно більше ітерацій для зближення. Проте навіть при далеких від істини початкових оцінках результати можуть бути задовільними за великого n, адже великий набір даних компенсує невдалий старт. Однак при малих значеннях n невдалий вибір початкових параметрів значно ускладнює отримання точних оцінок.\n",
        "\n",
        "Час зближення залежить від обсягу даних. Для невеликих n алгоритм збігається швидше, оскільки набір даних малий і зміни параметрів на кожній ітерації значні. Для великих n алгоритм потребує більше ітерацій через те, що обсяг даних уповільнює адаптацію параметрів.\n",
        "\n",
        "Щодо оцінок параметрів, середні mu_k відновлюються точніше, ніж ймовірності p_k. Особливо добре оцінюються середні для компонентів із високими ймовірностями, наприклад, mu_3 при p_3 = 0.4. Натомість алгоритм має труднощі з правильною оцінкою малих ймовірностей, таких як p_0 = 0.1, навіть за великого обсягу даних.\n",
        "\n",
        "Загалом, алгоритм демонструє хорошу поведінку для великих обсягів даних (n >= 1000), оскільки статистична репрезентативність вибірки допомагає отримати точні оцінки параметрів. Проте при малих обсягах даних або неправильних початкових оцінках результати можуть бути неточними, особливо для ймовірностей. Для підвищення ефективності бажано використовувати достатньо великі n, а початкові оцінки слід обирати якомога ближче до очікуваних значень. Це дозволить прискорити зближення алгоритму та підвищити точність його результатів."
      ],
      "metadata": {
        "id": "QPe2SiPujAzu"
      }
    }
  ]
}
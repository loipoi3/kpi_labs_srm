{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aSL1rAMGqpGw"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "n = 1000  # Number of random variables\n",
        "possible_means = np.array([0, 1, 2, 3])  # Possible means\n",
        "probabilities = np.array([(k + 1) / 10 for k in possible_means])  # Probabilities\n",
        "probabilities /= probabilities.sum()  # Normalize to ensure sum of probabilities is 1\n",
        "\n",
        "# Generate means according to probabilities\n",
        "random_means = np.random.choice(possible_means, size=n, p=probabilities)\n",
        "\n",
        "# Generate normal random variables with the selected means and variance 1\n",
        "random_variables = np.random.normal(loc=random_means, scale=1, size=n)\n",
        "\n",
        "\n",
        "def self_learning_algorithm_verbose(random_variables, tol=0.001):\n",
        "    # Initialize probabilities and means\n",
        "    ak = np.random.choice(np.arange(4), size=4)  # Random initial means\n",
        "    pK = np.ones(4) / 4  # Uniform initial probabilities\n",
        "\n",
        "    previous_pK = np.zeros_like(pK)  # To track changes in probabilities\n",
        "    iteration = 0  # Iteration counter\n",
        "\n",
        "    print(f\"Initial ak: {ak}\")\n",
        "    print(f\"Initial pK: {pK}\")\n",
        "\n",
        "    # Self-learning loop\n",
        "    while np.any(np.abs(pK - previous_pK) > tol):\n",
        "        iteration += 1\n",
        "        previous_pK = pK.copy()\n",
        "\n",
        "        # E-step: responsibilities (likelihood of each k for each observation)\n",
        "        responsibilities = np.array([\n",
        "            pK[k] * np.exp(-0.5 * (random_variables - ak[k])**2) / np.sqrt(2 * np.pi)\n",
        "            for k in range(4)\n",
        "        ])\n",
        "        responsibilities /= responsibilities.sum(axis=0)\n",
        "\n",
        "        # M-step: update means (ak) and probabilities (pK)\n",
        "        ak = np.sum(responsibilities * random_variables, axis=1) / np.sum(responsibilities, axis=1)\n",
        "        pK = responsibilities.mean(axis=1)\n",
        "\n",
        "        # Print iteration details\n",
        "        print(f\"Iteration {iteration}\")\n",
        "        print(f\"Updated ak: {ak}\")\n",
        "        print(f\"Updated pK: {pK}\")\n",
        "        print(f\"Change in pK: {np.abs(pK - previous_pK)}\\n\")\n",
        "\n",
        "    return ak, pK\n",
        "\n",
        "\n",
        "# Run the verbose algorithm\n",
        "ak_estimates, pK_estimates = self_learning_algorithm_verbose(random_variables)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoutkgbOrS1p",
        "outputId": "6fd47dc3-7a43-4d51-fbf1-24438ce0a572"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial ak: [2 2 3 1]\n",
            "Initial pK: [0.25 0.25 0.25 0.25]\n",
            "Iteration 1\n",
            "Updated ak: [2.09065487 2.09065487 3.17369855 0.74144307]\n",
            "Updated pK: [0.23524694 0.23524694 0.27624215 0.25326396]\n",
            "Change in pK: [0.01475306 0.01475306 0.02624215 0.00326396]\n",
            "\n",
            "Iteration 2\n",
            "Updated ak: [2.14655542 2.14655542 3.20185514 0.59352601]\n",
            "Updated pK: [0.22771879 0.22771879 0.28650391 0.2580585 ]\n",
            "Change in pK: [0.00752815 0.00752815 0.01026175 0.00479454]\n",
            "\n",
            "Iteration 3\n",
            "Updated ak: [2.18358545 2.18358545 3.18689987 0.53065396]\n",
            "Updated pK: [0.2249928  0.2249928  0.29126864 0.25874576]\n",
            "Change in pK: [0.00272599 0.00272599 0.00476473 0.00068726]\n",
            "\n",
            "Iteration 4\n",
            "Updated ak: [2.20747541 2.20747541 3.16425065 0.50340401]\n",
            "Updated pK: [0.22390309 0.22390309 0.2937534  0.25844042]\n",
            "Change in pK: [0.00108971 0.00108971 0.00248476 0.00030534]\n",
            "\n",
            "Iteration 5\n",
            "Updated ak: [2.22418767 2.22418767 3.14406941 0.49057065]\n",
            "Updated pK: [0.22319274 0.22319274 0.29534529 0.25826923]\n",
            "Change in pK: [0.00071035 0.00071035 0.00159189 0.00017118]\n",
            "\n",
            "Iteration 6\n",
            "Updated ak: [2.23687124 2.23687124 3.12789457 0.48431469]\n",
            "Updated pK: [0.22250984 0.22250984 0.29652731 0.25845302]\n",
            "Change in pK: [0.0006829  0.0006829  0.00118202 0.00018378]\n",
            "\n",
            "Iteration 7\n",
            "Updated ak: [2.24710496 2.24710496 3.11517335 0.48156627]\n",
            "Updated pK: [0.2217804  0.2217804  0.2974781  0.25896109]\n",
            "Change in pK: [0.00072944 0.00072944 0.00095079 0.00050808]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Алгоритм самонавчання почався з початкових значень \\(a_k = [2, 2, 3, 1]\\) та \\(p_k = [0.25, 0.25, 0.25, 0.25]\\), що відображає рівномірний розподіл ймовірностей між чотирма компонентами. На першій ітерації алгоритм оновив середні значення компонентів до \\(a_k = [2.0907, 2.0907, 3.1737, 0.7414]\\), що демонструє поступове наближення до центрів кластерів даних. Ймовірності також зазнали змін: \\(p_k = [0.2352, 0.2352, 0.2762, 0.2533]\\). Найбільше зростання спостерігалося для третього компонента (\\(+0.0262\\)), що свідчить про більшу щільність даних у цій області.\n",
        "\n",
        "На другій ітерації середні значення знову змінилися, тепер \\(a_k = [2.1466, 2.1466, 3.2019, 0.5935]\\), причому третій компонент продовжує рухатися до більшої точності. Ймовірності \\(p_k\\) стали \\( [0.2277, 0.2277, 0.2865, 0.2581]\\), із найбільшим збільшенням для третього компонента (\\(+0.0103\\)) та невеликим зростанням для четвертого компонента.\n",
        "\n",
        "До третьої ітерації середні значення стали \\(a_k = [2.1836, 2.1836, 3.1869, 0.5307]\\), тоді як ймовірності \\(p_k\\) зросли до \\( [0.2250, 0.2250, 0.2913, 0.2587]\\). Зміни стали менш вираженими: найбільше зростання знову демонструє третій компонент (\\(+0.0048\\)), що свідчить про поступове наближення до стабільного стану.\n",
        "\n",
        "На четвертій ітерації оновлення \\(a_k\\) дало значення \\( [2.2075, 2.2075, 3.1643, 0.5034]\\), а ймовірності \\(p_k = [0.2239, 0.2239, 0.2938, 0.2584]\\). Зміни в \\(p_k\\) стали ще меншими (\\(+0.0025\\) для третього компонента).\n",
        "\n",
        "До п’ятої ітерації середні стали \\( [2.2242, 2.2242, 3.1441, 0.4906]\\), а ймовірності \\(p_k = [0.2232, 0.2232, 0.2953, 0.2583]\\). Третій компонент продовжує зростати, але зміни (\\(+0.0016\\)) стали дуже малими.\n",
        "\n",
        "Шоста ітерація оновила середні до \\(a_k = [2.2369, 2.2369, 3.1279, 0.4843]\\), а ймовірності \\(p_k = [0.2225, 0.2225, 0.2965, 0.2585]\\). Зміни знову зменшилися, до \\(+0.0012\\) для третього компонента.\n",
        "\n",
        "На сьомій ітерації алгоритм дав середні \\(a_k = [2.2471, 2.2471, 3.1152, 0.4816]\\) і ймовірності \\(p_k = [0.2218, 0.2218, 0.2975, 0.2590]\\). Зміни стали мінімальними (\\(+0.0010\\) для третього компонента і \\(+0.0005\\) для четвертого), що свідчить про наближення до умов зупинки.\n",
        "\n",
        "Таким чином, алгоритм поступово налаштовувався на структуру даних, зменшуючи зміни в параметрах з кожною ітерацією. Третій компонент мав найвищу щільність даних, що відображено в його найбільшому зростанні ймовірності. Процес демонструє стабільну збіжність і ефективність у знаходженні оптимальних параметрів."
      ],
      "metadata": {
        "id": "GZ1GY_4ZxwUJ"
      }
    }
  ]
}